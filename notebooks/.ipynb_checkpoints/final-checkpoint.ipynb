{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e040901d-f226-4188-8319-70e800c025fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/jupyter/detection doublons/src\") # for adding the directory \"src\"\n",
    "from mesures import *\n",
    "from custom_processing import *\n",
    "from visualization import *\n",
    "from custom_hierachical_clustering import *\n",
    "import pickle\n",
    "\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Tuple, Union\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import gravis as gv\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75984a75-7925-45a8-925c-d2e423312ee5",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a46e75b-ad10-4c23-b82c-7074200fc5db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58005, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data=pd.read_csv(\"/home/jupyter/detection doublons/data/gold_fornissor.csv\")\n",
    "data=original_data.copy()\n",
    "# n_data=data.drop([\"id\", \"confidence_country\",\"confidence_city\", \"Latitude\",\"Longitude\"], axis=1).copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b4d2c-e54c-4bda-9e61-9b8ef63c4543",
   "metadata": {},
   "source": [
    "### First step processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f021cdb1-f5d2-4030-9d37-a08a7ae9f554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop =[\"id\", \"confidence_country\",\"confidence_city\", \"Latitude\",\"Longitude\"]\n",
    "custom_processing = Custom_Processing(drop_cols=cols_to_drop, dataframe=data)\n",
    "custom_processing.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f17cde-9c8d-425c-88a8-1eeee9c6f51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numbers1 = custom_processing.cont_figure_dataframe\n",
    "\n",
    "duplication1 = custom_processing.duplicates_dataframe\n",
    "duplication1 = duplication1.rename(columns = {\"tiern_name_preprocessed\": \"suggested_name\"})\n",
    "\n",
    "special1 = custom_processing.special_char_dataframe\n",
    "special1[\"suggested_name\"] = special1[\"tiern_name_preprocessed\"]\n",
    "\n",
    "others1 = custom_processing.others_dataframe\n",
    "others1 = others1.rename(columns = {\"tiern_name_preprocessed\": \"suggested_name\"})\n",
    "\n",
    "processed1 = custom_processing.dataframe\n",
    "processed1[\"suggested_name\"] = processed1[\"tiern_name_preprocessed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbd8f4-598b-4c3c-9c83-826bb41e6b91",
   "metadata": {},
   "source": [
    "#### Cheicking\n",
    "Cheick if no data point are lose during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a8d2a2-75c7-4daa-b1c9-d53a436463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert numbers1.shape[0]  + duplication1.shape[0] + special1.shape[0] + others1.shape[0] + processed1.shape[0] == data.shape[0], \"Some data points have been losen during processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1e1f3-b998-41d8-a0a0-9925be0c913a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Second step processing\n",
    "#### Translation \n",
    "special characters dataset contain tiern name with different language which are different from english.<br>\n",
    "To use this part of dta in the raining process, we translate those language into frecnh and then  concatenate it with the previous processed dataset which \n",
    "do not contain duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff8b6d6-bea4-4a0f-aa24-b89c68d82f63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(436, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special1 = special1.drop([\"tiern_name_preprocessed\"], axis=1)\n",
    "special1 = translation(special1, special_char_cols=['tiern_name', 'tiern_plant'])\n",
    "special1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687144bf-8b92-4f6e-bbe0-bde9d8460191",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Processing2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589640dc-a711-44e0-9d41-52b9ef7633bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 311.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 213.14it/s]\n"
     ]
    }
   ],
   "source": [
    "custom_processing1 = Custom_Processing(dataframe=special1)\n",
    "custom_processing1.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edbeab-61aa-4468-aeb1-ad11518158b4",
   "metadata": {},
   "source": [
    "#### cheicking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496919f0-027d-4716-b667-f1c7d22b31b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numbers2 = custom_processing1.cont_figure_dataframe\n",
    "\n",
    "duplication2 = custom_processing1.duplicates_dataframe\n",
    "duplication2 = duplication2.rename(columns = {\"tiern_name_preprocessed\": \"suggested_name\"})\n",
    "\n",
    "special2 = custom_processing1.special_char_dataframe\n",
    "special2[\"suggested_name\"] = special2[\"tiern_name_preprocessed\"]\n",
    "\n",
    "others2 = custom_processing1.others_dataframe\n",
    "others2 = others2.rename(columns = {\"tiern_name_preprocessed\": \"suggested_name\"})\n",
    "\n",
    "processed2 = custom_processing1.dataframe\n",
    "processed2[\"suggested_name\"] = processed2[\"tiern_name_preprocessed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787c5343-5022-4ad2-a1fa-112e4e24d65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert numbers2.shape[0]  + duplication2.shape[0] + special2.shape[0] + others2.shape[0] + processed2.shape[0] == special1.shape[0],\"Some data points have been losen during processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2e3f8-4463-4582-a044-8305bbaba52a",
   "metadata": {},
   "source": [
    "#### Defining the référence dataset\n",
    "This is the datframe on which some occurence of tiern name of processed data will be seeked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b240463-af11-4018-9f21-647c637077d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54952, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = pd.concat((custom_processing.ref_dataframe, custom_processing1.ref_dataframe), axis=0)\n",
    "ref.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2086df-8341-4953-a299-d07b74e6451b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train data\n",
    "Forming the training dataset. It consist of the first processed dataset and the second one (after translating it to the correct langue and processing it too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8347c06b-531f-4933-89fd-b4c0a8d0b676",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40661, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat((processed1, processed2))\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12981a44-2841-444c-90be-15933e1d5a17",
   "metadata": {},
   "source": [
    "There can be some duplication whithin the training dataset after concatenation, beacause after translation there can be some data point present in the both parts.<br>\n",
    "So those duplication should be drop to have a clean train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2d0830-b6b2-49f9-8591-3bd391d0c8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40648, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplication3 = train_data.drop(index= train_data.drop_duplicates(['tiern_location_state_city', 'Country', 'tiern_name_preprocessed']).index)\n",
    "train_data.drop_duplicates(['tiern_location_state_city', 'Country', 'tiern_name_preprocessed'], inplace=True)\n",
    "duplication3[\"category\"]=\"duplicates\"\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b77df8-26bc-47aa-aeba-cb438630afbd",
   "metadata": {},
   "source": [
    "#### Training\n",
    "During training process somme duplicates can be detect and there can also be some clusters which contain only a single data point. <br>\n",
    "For the purpose of suggesting the correct name within a cluster, all single data point whithin clusters are save whithin the same dataframe and the other hand all detected duplicates are save within dataframe different from the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9625d-0c84-481d-b654-1cf380099146",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3328/3388 [04:17<00:05, 10.71it/s]"
     ]
    }
   ],
   "source": [
    "distance_threshold = 0.1\n",
    "model_city = Agglomerative_Hierachical_Clustering(distance_threshold=distance_threshold, linkage =\"average\", metric = damerau_levenshtein_distance, normalize_metric=True)\n",
    "path1=f\"/home/jupyter/detection doublons/data/final/tatal_{str(distance_threshold).replace('.', '_')}_doublons_.csv\"\n",
    "path2=f\"/home/jupyter/detection doublons/data/final/tatal_{str(distance_threshold).replace('.', '_')}_singles_.csv\"\n",
    "# model_city.fit_per_city(traain_data.groupby([\"Country\", \"tiern_location_state_city\"]).filter(lambda x : len(x) <=4 and len(x) >2), path1=path1, path2= path2\n",
    "model_city.fit_per_city(train_data, path1=path1, path2= path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7b5f7-7c9f-4c18-9c9f-e411ade9d4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(path1)\n",
    "single_dataframe = pd.read_csv(path2)\n",
    "dataframe1 = dataframe.drop_duplicates(['country', 'city', 'duplication_id'])\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf23601-4ddf-4947-8cd2-f7e2e55c6f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert single_dataframe.shape[0] + dataframe.shape[0] == train_data.shape[0], f\"single_dataframe.shape[0] + dataframe.shape[0] = {single_dataframe.shape[0] + dataframe.shape[0]} is differente from train_data.shape[0] = {train_data.shape[0]}, whiche is abnormal\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3b670-fdf1-43e6-8091-6925f1a2ed85",
   "metadata": {},
   "source": [
    "#### Processing the dupliction dataframe\n",
    "##### Occurrence determination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b3cf9-f23a-4d82-b87d-512944ac2bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inter_dataframe = occurrences(ref_dataframe = ref, processing_dataframe = dataframe, col ='tiern_name_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c5ce4-c5f3-4b28-8b09-faa4ea9097c4",
   "metadata": {},
   "source": [
    "##### Making suggestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119b429-514e-4572-8a98-23faadec432b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suggested_dataframe = inter_dataframe.groupby(\"duplication_id\").apply(suggest_group_name,tiern_name='tiern_name_preprocessed').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7ae34-5bbf-4b69-a5d9-a65d64558c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert inter_dataframe.shape[0] == dataframe.shape[0], f\"The occurence dataframe must have the same size as the duplication dataframe, but one have inter_dataframe.shape[0] = {inter_dataframe.shape[0]} and  dataframe.shape[0] = { dataframe.shape[0]}\"\n",
    "assert suggested_dataframe.shape[0] == dataframe.shape[0], f\"The suggested dataframe must have the same size as the duplication dataframe, but one have suggested_dataframe.shape[0] = {suggested_dataframe.shape[0]} and  dataframe.shape[0] = { dataframe.shape[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cac9f8-aeb0-4b93-8c61-91bcf075939c",
   "metadata": {},
   "source": [
    "### final processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99ef29-3e00-4564-b305-54d7986e8691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_dataframe = suggested_dataframe.drop([\"duplication_id\", \"occurrence\"], axis=1)\n",
    "final_dataframe[\"category\"] = \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8956963-f2ab-43c2-acdc-78ebd9607ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert final_dataframe.shape[0] == dataframe.shape[0], f\"final_dataframe must have the same size as the duplication dataframe, but one have final_dataframe.shape[0] = {final_dataframe.shape[0]} and  dataframe.shape[0] = { dataframe.shape[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f20585-f150-49b2-b134-e94f3cbbc673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_dataframe4 = final_dataframe.drop_duplicates([\"country\", \"city\", \"suggested_name\"])\n",
    "duplication4 = final_dataframe.drop(index=processed_dataframe4.index, axis=0)\n",
    "duplication4[\"category\"] =\"duplicates\"\n",
    "\n",
    "single_dataframe = pd.read_csv(path2)\n",
    "single_dataframe[\"category\"] = \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf912f-653e-404f-9942-6fa7f46e91bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f77b99-536c-414d-b853-57623f753951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single_dataframe.rename(columns={'suggest' : \"suggested_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aec9af-4e74-477f-a778-389a938909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(list(others1.columns) )\n",
    "others2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1620176-21e6-4705-b370-3695132bcc36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Gather others data\n",
    "others = pd.concat((others1, others2, special2))\n",
    "# others[\"category\"] = \"other\"\n",
    "\n",
    "# others.drop([\"chars\"], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8ddb3-b014-4af1-8ca9-91b61be886d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "others = pd.concat((others1, others2, special2))\n",
    "others = others.rename(columns={\"Country\" : 'country',\"tiern_location_state_city\": 'city'}) # Renaming some columns\n",
    "others = others[list(single_dataframe.columns)]  # order the columns in the disired order\n",
    "\n",
    "numbers1 = numbers1.rename( columns={\"Country\" : 'country',\"tiern_location_state_city\": 'city'})  # Renaming some columns\n",
    "numbers1 = numbers1[list(single_dataframe.columns)] # order the columns in the disired order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafeeec-313f-4071-8a0f-ca2f15404c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplication5 = pd.concat((numbers1.drop(index = numbers1.drop_duplicates([\"country\", \"city\", \"suggested_name\"]).index),\n",
    "                          others.drop(index = others.drop_duplicates([\"country\", \"city\", \"suggested_name\"]).index)), axis=0)\n",
    "\n",
    "numbers1.drop_duplicates([\"country\", \"city\", \"suggested_name\"], inplace=True)\n",
    "others.drop_duplicates([\"country\", \"city\", \"suggested_name\"], inplace=True)\n",
    "duplication5['category'] = \"duplicates\"\n",
    "# duplication4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f3632-2028-4440-ab64-fe264abaad73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplication_dataframe = pd.concat((duplication1, duplication2, duplication3), axis=0)\n",
    "duplication_dataframe.drop(['chars'], axis= 1, inplace=True)\n",
    "duplication_dataframe = duplication_dataframe.rename( columns={\"Country\" : 'country',\"tiern_location_state_city\": 'city'})\n",
    "duplication_dataframe = duplication_dataframe[list(single_dataframe.columns)]\n",
    "duplication_dataframe = pd.concat((duplication_dataframe, duplication4,duplication5), axis = 0)\n",
    "\n",
    "final_processed_dataframe= pd.concat((single_dataframe,processed_dataframe4, others, numbers1, duplication_dataframe), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3832b76-780d-4c44-a1dc-d14a23a9b203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_processed_dataframe['similarity_score']= round(1- distance_threshold, 1)\n",
    "final_processed_dataframe.drop([\"tiern_name_preprocessed\"], axis=1)\n",
    "path=f\"/home/jupyter/detection doublons/data/final/tatal_{str(distance_threshold).replace('.', '_')}_total.csv\"\n",
    "final_processed_dataframe.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450e85e-1f0e-487f-adc9-8d59e97d4413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert final_processed_dataframe.shape[0] == data.shape[0], f\"final_processed_dataframe.shape[0] = {final_processed_dataframe.shape[0]} must be equal to data.shape[0] = {data.shape[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d93318-60fb-46c3-96bc-f8086830ad61",
   "metadata": {},
   "source": [
    "###  Duplication percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b8f77-f97f-4d6e-91ac-35ed3ebf2209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_processed_dataframe.loc[final_processed_dataframe[\"category\"]==\"duplicates\", :].shape[0]/58005 *100 + \"% of duplicates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794c107-da9f-4f6d-af61-ea44f1903954",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_distance = [round(0.1 * i, 1) for i in range(1, 6)]\n",
    "total = None \n",
    "\n",
    "for distance_threshold in list_of_distance:\n",
    "    path=f\"/home/jupyter/detection doublons/data/final/tatal_{str(distance_threshold).replace('.', '_')}_total.csv\"\n",
    "    dataframe= pd.read_csv(path)\n",
    "    \n",
    "    total = pd.concat((total, dataframe), axis=0)\n",
    "\n",
    "print(total.shape)\n",
    "total[\"suggested_name\"]= total[\"suggest\"]\n",
    "total[\"similarity_score\"] = round(1 - total[\"distance_threshold\"], 1)\n",
    "total.drop(columns =[\"suggest\",\"distance_threshold\"], inplace=True)\n",
    "total.shape[0] == 58005 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8f7e4-80ca-45d8-a8ac-fb45a9ba9e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(total['similarity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cfeae7-5805-4e6d-95da-5b2253b2dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data=pd.read_csv(\"/home/jupyter/detection doublons/data/gold_fornissor.csv\")\n",
    "data=original_data.copy()\n",
    "\n",
    "data[\"tiern_location_state_city\"] = data[\"tiern_location_state_city\"].replace({\n",
    "    'Frankfurt (Oder)': 'frankfurt oder', \n",
    "    'Charleville-Mézières': 'Charleville Mézières'\n",
    "})\n",
    "\n",
    "data[\"tiern_location_state_city\"] = data[\"tiern_location_state_city\"].str.lower().str.strip()\n",
    "data[\"Country\"] = data[\"Country\"].str.lower().str.strip()\n",
    "\n",
    "data = data.rename(columns={\n",
    "    \"tiern_location_state_city\" : \"city\",\n",
    "    \"Country\" : \"country\",\n",
    "    \"tiern_name\" : \"tiern_name\",\n",
    "    \"tiern_plant\" : \"tiern_plant\",\n",
    "    \"Latitude\" : \"latitude\",\n",
    "    \"Longitude\" : \"longitude\",\n",
    "    \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82353fa7-e857-48ef-9be6-4ef3015818e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "toatl = total.merge(data[['country', 'city','longitude', 'latitude']].drop_duplicates(['country', 'city']), on=['country', 'city'], how='left')\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d68b6-7cfb-4324-bbfd-d4e76d7c52b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=f\"/home/jupyter/detection doublons/data/final/tatal_total.csv\"\n",
    "total.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7de0a-e417-464e-a82c-78c41c07a9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_threshold =0.3\n",
    "path=f\"/home/jupyter/detection doublons/data/final/tatal_{str(distance_threshold).replace('.', '_')}_total.csv\"\n",
    "dataframe= pd.read_csv(path)\n",
    "print(dataframe.shape)\n",
    "print(data.shape)\n",
    "manq = data.loc[~dataframe['city'].isin(list(data['city'])) , :]\n",
    "manq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d877d-1bdc-4761-8264-e0617a75cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
